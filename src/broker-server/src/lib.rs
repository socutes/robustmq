// Copyright 2023 RobustMQ Team
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use crate::{connection::network_connection_gc, grpc::start_grpc_server};
use admin_server::{
    server::AdminServer,
    state::{HttpState, MQTTContext, StorageEngineContext},
};
use broker_core::{
    cache::BrokerCacheManager,
    heartbeat::{check_meta_service_status, register_node, report_heartbeat},
};
use common_base::{
    error::common::CommonError,
    role::{is_broker_node, is_engine_node, is_meta_node},
    runtime::{
        create_runtime, resolve_broker_worker_threads, resolve_meta_worker_threads,
        resolve_server_worker_threads,
    },
};
use common_config::{
    broker::broker_config, config::BrokerConfig, storage::memory::StorageDriverMemoryConfig,
};
use common_metrics::{core::server::register_prometheus_export, init_metrics};
use connector::{manager::ConnectorManager, start_connector};
use delay_message::manager::DelayMessageManager;
use delay_task::{manager::DelayTaskManager, start_delay_task_manager_thread};
use grpc_clients::pool::ClientPool;
use meta_service::{
    core::cache::MetaCacheManager as PlacementCacheManager,
    raft::{manager::MultiRaftManager, route::DataRoute},
    MetaServiceServer, MetaServiceServerParams,
};
use mqtt_broker::{
    broker::{MqttBrokerServer, MqttBrokerServerParams},
    core::{cache::MQTTCacheManager as MqttCacheManager, retain::RetainMessageManager},
    security::AuthDriver,
    storage::session::SessionBatcher,
    subscribe::{manager::SubscribeManager, PushManager},
};
use network_server::common::connection_manager::ConnectionManager as NetworkConnectionManager;
use node_call::NodeCallManager;
use pprof_monitor::pprof_monitor::start_pprof_monitor;
use rate_limit::RateLimiterManager;
use rocksdb_engine::{
    metrics::mqtt::MQTTMetricsCache,
    rocksdb::RocksDBEngine,
    storage::family::{column_family_list, storage_data_fold},
};
use schema_register::schema::SchemaRegisterManager;
use std::{
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
    thread::sleep,
    time::Duration,
};
use storage_adapter::driver::StorageDriverManager;
use storage_engine::{
    clients::manager::ClientConnectionManager, commitlog::memory::engine::MemoryStorageEngine,
    commitlog::rocksdb::engine::RocksDBStorageEngine, core::cache::StorageCacheManager,
    filesegment::write::WriteManager, group::OffsetManager, handler::adapter::StorageEngineHandler,
    StorageEngineParams, StorageEngineServer,
};
use system_info::{start_monitor, start_runtime_monitor};
use tokio::{runtime::Runtime, signal, sync::broadcast};
use tracing::{error, info};

mod cluster_service;
pub mod common;
mod connection;
mod grpc;

pub struct BrokerServer {
    server_runtime: Runtime,
    /// Dedicated runtime for Raft tasks; Raft::new() is called inside
    /// meta_runtime.block_on() so all openraft internal tasks are spawned here,
    /// isolated from the gRPC server_runtime.
    meta_runtime: Runtime,
    /// Dedicated runtime for MQTT broker tasks; build_broker_mqtt_params is called
    /// inside broker_runtime.block_on() so tasks spawned during construction
    /// (e.g. RetainMessageManager's send thread) land here, not on server_runtime.
    broker_runtime: Runtime,
    place_params: MetaServiceServerParams,
    mqtt_params: MqttBrokerServerParams,
    engine_params: StorageEngineParams,
    client_pool: Arc<ClientPool>,
    rocksdb_engine_handler: Arc<RocksDBEngine>,
    rate_limiter_manager: Arc<RateLimiterManager>,
    connection_manager: Arc<NetworkConnectionManager>,
    broker_cache: Arc<BrokerCacheManager>,
    offset_manager: Arc<OffsetManager>,
    delay_task_manager: Arc<DelayTaskManager>,
    node_call_manager: Arc<NodeCallManager>,
    config: BrokerConfig,
}

impl Default for BrokerServer {
    fn default() -> Self {
        Self::new()
    }
}

impl BrokerServer {
    pub fn new() -> Self {
        init_metrics();
        let config = broker_config();
        let client_pool = Arc::new(ClientPool::new(config.grpc_client.channels_per_address));
        let rocksdb_engine_handler = Arc::new(RocksDBEngine::new(
            &storage_data_fold(&config.rocksdb.data_path),
            config.rocksdb.max_open_files,
            column_family_list(),
        ));
        let rate_limiter_manager = Arc::new(RateLimiterManager::new());
        let server_worker_threads =
            resolve_server_worker_threads(config.runtime.server_worker_threads);
        let meta_worker_threads = resolve_meta_worker_threads(config.runtime.meta_worker_threads);
        let broker_worker_threads =
            resolve_broker_worker_threads(config.runtime.broker_worker_threads);

        let server_runtime = create_runtime("server-runtime", server_worker_threads);
        let broker_cache = Arc::new(BrokerCacheManager::new(config.clone()));
        let connection_manager = Arc::new(NetworkConnectionManager::new());

        let (main_stop_send, _) = broadcast::channel(2);

        let offset_manager = Arc::new(OffsetManager::new(
            client_pool.clone(),
            rocksdb_engine_handler.clone(),
            config.storage_offset.enable_cache,
        ));

        let node_call_manager = Arc::new(NodeCallManager::new(
            client_pool.clone(),
            broker_cache.clone(),
        ));

        // storage adapter driver (sync, no async needed)
        let engine_params = BrokerServer::build_storage_engine_params(
            client_pool.clone(),
            rocksdb_engine_handler.clone(),
            broker_cache.clone(),
            connection_manager.clone(),
            offset_manager.clone(),
        );

        // Create meta_runtime here so that Raft::new() (inside build_meta_service) is
        // called within meta_runtime.block_on().  openraft spawns ~9 internal tasks via
        // tokio::spawn() during Raft::new(); those calls resolve against the *current*
        // runtime context, so they all land on meta_runtime instead of server_runtime.
        let meta_runtime = create_runtime("meta-runtime", meta_worker_threads);
        let mqtt_om = offset_manager.clone();
        let mqtt_seh = engine_params.storage_engine_handler.clone();
        let storage_driver_manager = meta_runtime.block_on(async move {
            match StorageDriverManager::new(mqtt_om.clone(), mqtt_seh).await {
                Ok(storage) => Arc::new(storage),
                Err(e) => {
                    error!("Failed to build message storage driver: {}", e);
                    std::process::exit(1);
                }
            }
        });

        let delay_task_manager: Arc<DelayTaskManager> = Arc::new(DelayTaskManager::new(
            client_pool.clone(),
            storage_driver_manager.clone(),
            1,
            10,
        ));

        // Run build_meta_service on meta_runtime so all openraft internal tasks
        // (core loop, log IO, state machine worker, etc.) are isolated from the
        // gRPC server_runtime, eliminating task-scheduler contention.
        let meta_params = meta_runtime.block_on(BrokerServer::build_meta_service(
            client_pool.clone(),
            rocksdb_engine_handler.clone(),
            delay_task_manager.clone(),
            node_call_manager.clone(),
            broker_cache.clone(),
        ));

        // Create broker_runtime here so that tasks spawned during MQTT param
        // construction (e.g. RetainMessageManager's send thread) land on
        // broker_runtime rather than server_runtime.
        let broker_runtime = create_runtime("broker-runtime", broker_worker_threads);

        let mqtt_om = offset_manager.clone();
        let mqtt_cp = client_pool.clone();
        let mqtt_bc = broker_cache.clone();
        let mqtt_re = rocksdb_engine_handler.clone();
        let mqtt_cm = connection_manager.clone();
        let mqtt_stop = main_stop_send.clone();
        let mqttt_sdm = storage_driver_manager.clone();

        let mqtt_params = broker_runtime.block_on(async move {
            match BrokerServer::build_broker_mqtt_params(
                mqtt_cp, mqtt_bc, mqtt_re, mqtt_cm, mqttt_sdm, mqtt_om, mqtt_stop,
            )
            .await
            {
                Ok(params) => params,
                Err(e) => {
                    error!("Failed to build MQTT broker params: {}", e);
                    std::process::exit(1);
                }
            }
        });

        BrokerServer {
            broker_cache,
            server_runtime,
            meta_runtime,
            broker_runtime,
            place_params: meta_params,
            engine_params,
            config: config.clone(),
            mqtt_params,
            client_pool,
            delay_task_manager,
            rocksdb_engine_handler,
            rate_limiter_manager,
            connection_manager,
            node_call_manager,
            offset_manager,
        }
    }

    pub fn start(&self) {
        // start grpc server
        let place_params = self.place_params.clone();
        let mqtt_params = self.mqtt_params.clone();
        let engine_params = self.engine_params.clone();
        let broker_cache = self.broker_cache.clone();
        let grpc_port = self.config.grpc_port;

        let grpc_ready = Arc::new(AtomicBool::new(false));
        let grpc_ready_check = grpc_ready.clone();
        self.server_runtime.spawn(Box::pin(async move {
            if let Err(e) =
                start_grpc_server(place_params, mqtt_params, engine_params, grpc_port).await
            {
                error!("Failed to start GRPC server: {}", e);
                std::process::exit(1);
            }
        }));

        // Start Admin Server
        let state = Arc::new(HttpState {
            client_pool: self.client_pool.clone(),
            connection_manager: self.mqtt_params.connection_manager.clone(),
            mqtt_context: MQTTContext {
                cache_manager: self.mqtt_params.cache_manager.clone(),
                subscribe_manager: self.mqtt_params.subscribe_manager.clone(),
                metrics_manager: self.mqtt_params.metrics_cache_manager.clone(),
                connector_manager: self.mqtt_params.connector_manager.clone(),
                schema_manager: self.mqtt_params.schema_manager.clone(),
                retain_message_manager: self.mqtt_params.retain_message_manager.clone(),
                push_manager: self.mqtt_params.push_manager.clone(),
                storage_driver_manager: self.mqtt_params.storage_driver_manager.clone(),
            },
            engine_context: StorageEngineContext {
                engine_adapter_handler: self.engine_params.storage_engine_handler.clone(),
                cache_manager: self.engine_params.cache_manager.clone(),
            },
            rocksdb_engine_handler: self.rocksdb_engine_handler.clone(),
            broker_cache,
            rate_limiter_manager: self.rate_limiter_manager.clone(),
            storage_driver_manager: self.mqtt_params.storage_driver_manager.clone(),
        });

        let http_port = self.config.http_port;
        self.server_runtime.spawn(async move {
            let admin_server = AdminServer::new();
            admin_server.start(http_port, state).await;
        });

        // check grpc server ready
        self.check_grpc_server_ready(grpc_port, grpc_ready_check);

        // start pprof server
        self.server_runtime.spawn(async move {
            let conf = broker_config();
            if conf.p_prof.enable {
                start_pprof_monitor(conf.p_prof.port, conf.p_prof.frequency).await;
            }
        });

        // start prometheus
        let prometheus_port = self.config.prometheus.port;
        if self.config.prometheus.enable {
            self.server_runtime.spawn(async move {
                register_prometheus_export(prometheus_port).await;
            });
        }

        self.wait_for_grpc_ready(&grpc_ready);

        let mut meta_stop_send = None;
        let mut mqtt_stop_send = None;
        let mut engine_stop_send = None;

        let config = broker_config();
        let monitor_interval_ms = config.prometheus.monitor_interval_ms;

        // start meta service
        // meta_runtime was created in new() so all Raft internal tasks already
        // live there; MetaServiceServer background tasks also spawn here.
        let (stop_send, _) = broadcast::channel(2);
        let place_params = self.place_params.clone();
        if is_meta_node(&config.roles) {
            meta_stop_send = Some(stop_send.clone());
            self.meta_runtime.spawn(Box::pin(async move {
                let mut pc = MetaServiceServer::new(place_params, stop_send.clone());
                pc.start().await;
            }));
        }

        // check meta service ready
        self.server_runtime.block_on(async {
            check_meta_service_status(self.client_pool.clone()).await;
        });

        let (stop_send, _) = broadcast::channel(2);

        // node call
        let node_call_manager = self.node_call_manager.clone();
        let raw_stop_send = stop_send.clone();
        self.server_runtime.spawn(async move {
            node_call_manager.start(raw_stop_send).await;
        });

        // delay task
        let raw_rocksdb_engine_handler = self.rocksdb_engine_handler.clone();
        let raw_broker_cache = self.broker_cache.clone();
        let raw_delay_task_manager = self.delay_task_manager.clone();
        let raw_node_call_manager = self.node_call_manager.clone();
        self.server_runtime.spawn(async move {
            if let Err(e) = start_delay_task_manager_thread(
                &raw_rocksdb_engine_handler,
                &raw_delay_task_manager,
                &raw_broker_cache,
                &raw_node_call_manager,
            )
            .await
            {
                error!("Failed to start DelayTask pop threads: {}", e);
                std::process::exit(1);
            }
        });

        // register node
        let raw_stop_send = stop_send.clone();
        self.server_runtime.block_on(async move {
            self.register_node(raw_stop_send.clone()).await;
        });

        // broker_runtime was created in new() so all MQTT and engine tasks
        // (including those spawned during construction) are on the same runtime.
        if is_engine_node(&config.roles) {
            engine_stop_send = Some(stop_send.clone());
            let server = StorageEngineServer::new(self.engine_params.clone(), stop_send);
            self.broker_runtime.spawn(Box::pin(async move {
                server.start().await;
            }));
            self.wait_for_engine_ready();
        }

        let (stop_send, _) = broadcast::channel(2);
        if is_broker_node(&config.roles) {
            mqtt_stop_send = Some(stop_send.clone());
            let server = MqttBrokerServer::new(self.mqtt_params.clone(), stop_send.clone());
            self.broker_runtime.spawn(Box::pin(async move {
                server.start().await;
            }));
        }

        // connection gc
        let connection_manager = self.connection_manager.clone();
        let raw_stop_send = stop_send.clone();
        self.server_runtime
            .spawn(async move { network_connection_gc(connection_manager, raw_stop_send).await });

        // offset flush thread
        let offset_cache = self.offset_manager.clone();
        let raw_stop_send = stop_send.clone();
        self.server_runtime.spawn(Box::pin(async move {
            offset_cache.offset_save_thread(raw_stop_send).await;
        }));

        // system resource monitor
        let raw_stop_send = stop_send.clone();
        self.server_runtime.spawn(async move {
            start_monitor(raw_stop_send, monitor_interval_ms).await;
        });

        // Tokio runtime metrics monitor
        let runtime_handles = vec![
            ("server".to_string(), self.server_runtime.handle().clone()),
            ("meta".to_string(), self.meta_runtime.handle().clone()),
            ("broker".to_string(), self.broker_runtime.handle().clone()),
        ];
        let raw_stop_send = stop_send.clone();
        self.server_runtime.spawn(async move {
            start_runtime_monitor(runtime_handles, raw_stop_send, monitor_interval_ms).await;
        });

        // connector
        let message_storage = self.mqtt_params.storage_driver_manager.clone();
        let connector_manager = self.mqtt_params.connector_manager.clone();
        let raw_stop_send = stop_send.clone();
        let client_poll = self.client_pool.clone();
        tokio::spawn(Box::pin(async move {
            start_connector(
                &client_poll,
                &message_storage,
                &connector_manager,
                &raw_stop_send,
            )
            .await;
        }));

        // awaiting stop
        self.awaiting_stop(meta_stop_send, mqtt_stop_send, engine_stop_send);
    }

    async fn build_meta_service(
        client_pool: Arc<ClientPool>,
        rocksdb_engine_handler: Arc<RocksDBEngine>,
        delay_task_manager: Arc<DelayTaskManager>,
        node_call_manager: Arc<NodeCallManager>,
        broker_cache: Arc<BrokerCacheManager>,
    ) -> MetaServiceServerParams {
        let cache_manager = Arc::new(PlacementCacheManager::new(rocksdb_engine_handler.clone()));

        let data_route = Arc::new(DataRoute::new(
            rocksdb_engine_handler.clone(),
            cache_manager.clone(),
            delay_task_manager.clone(),
            broker_cache.clone(),
        ));
        let raft_manager = Arc::new(
            match MultiRaftManager::new(
                client_pool.clone(),
                rocksdb_engine_handler.clone(),
                data_route,
            )
            .await
            {
                Ok(data) => data,
                Err(e) => {
                    error!("Failed to create MultiRaftManager: {}", e);
                    std::process::exit(1);
                }
            },
        );

        MetaServiceServerParams {
            cache_manager,
            rocksdb_engine_handler,
            client_pool,
            node_call_manager,
            raft_manager,
            delay_task_manager,
            broker_cache,
        }
    }

    async fn build_broker_mqtt_params(
        client_pool: Arc<ClientPool>,
        broker_cache: Arc<BrokerCacheManager>,
        rocksdb_engine_handler: Arc<RocksDBEngine>,
        connection_manager: Arc<NetworkConnectionManager>,
        storage_driver_manager: Arc<StorageDriverManager>,
        offset_manager: Arc<OffsetManager>,
        stop_sx: broadcast::Sender<bool>,
    ) -> Result<MqttBrokerServerParams, CommonError> {
        let cache_manager = Arc::new(MqttCacheManager::new(
            client_pool.clone(),
            broker_cache.clone(),
        ));
        let subscribe_manager = Arc::new(SubscribeManager::new());
        let connector_manager = Arc::new(ConnectorManager::new());
        let auth_driver = Arc::new(AuthDriver::new(cache_manager.clone(), client_pool.clone()));
        let delay_message_manager = Arc::new(
            DelayMessageManager::new(client_pool.clone(), storage_driver_manager.clone(), 5)
                .await?,
        );
        let metrics_cache_manager = Arc::new(MQTTMetricsCache::new(rocksdb_engine_handler.clone()));
        let schema_manager = Arc::new(SchemaRegisterManager::new());
        let retain_message_manager = RetainMessageManager::new(
            cache_manager.clone(),
            client_pool.clone(),
            connection_manager.clone(),
            stop_sx,
        );
        let push_manager = Arc::new(PushManager::new(
            cache_manager.clone(),
            storage_driver_manager.clone(),
            connection_manager.clone(),
            rocksdb_engine_handler.clone(),
            subscribe_manager.clone(),
            client_pool.clone(),
        ));

        let session_batcher = SessionBatcher::new();

        Ok(MqttBrokerServerParams {
            cache_manager,
            client_pool,
            session_batcher,
            storage_driver_manager,
            subscribe_manager,
            connection_manager,
            connector_manager,
            auth_driver,
            delay_message_manager,
            schema_manager,
            metrics_cache_manager,
            rocksdb_engine_handler,
            broker_cache,
            offset_manager,
            retain_message_manager,
            push_manager,
        })
    }

    fn build_storage_engine_params(
        client_pool: Arc<ClientPool>,
        rocksdb_engine_handler: Arc<RocksDBEngine>,
        broker_cache: Arc<BrokerCacheManager>,
        connection_manager: Arc<NetworkConnectionManager>,
        offset_manager: Arc<OffsetManager>,
    ) -> StorageEngineParams {
        let config = broker_config();

        let cache_manager = Arc::new(StorageCacheManager::new(broker_cache.clone()));
        let write_manager = Arc::new(WriteManager::new(
            rocksdb_engine_handler.clone(),
            cache_manager.clone(),
            client_pool.clone(),
            config.storage_runtime.io_thread_num,
        ));
        let memory_storage_engine = Arc::new(MemoryStorageEngine::new(
            rocksdb_engine_handler.clone(),
            cache_manager.clone(),
            StorageDriverMemoryConfig::default(),
        ));
        let rocksdb_storage_engine = Arc::new(RocksDBStorageEngine::new(
            cache_manager.clone(),
            rocksdb_engine_handler.clone(),
        ));

        let client_connection_manager =
            Arc::new(ClientConnectionManager::new(cache_manager.clone(), 4));

        let storage_engine_handler = Arc::new(StorageEngineHandler::new(
            storage_engine::handler::adapter::StorageEngineHandlerParams {
                cache_manager: cache_manager.clone(),
                client_pool: client_pool.clone(),
                memory_storage_engine: memory_storage_engine.clone(),
                rocksdb_storage_engine: rocksdb_storage_engine.clone(),
                client_connection_manager: client_connection_manager.clone(),
                rocksdb_engine_handler: rocksdb_engine_handler.clone(),
                write_manager: write_manager.clone(),
                offset_manager: offset_manager.clone(),
            },
        ));
        StorageEngineParams {
            cache_manager,
            client_pool,
            rocksdb_engine_handler,
            connection_manager,
            client_connection_manager,
            memory_storage_engine,
            rocksdb_storage_engine,
            write_manager,
            storage_engine_handler,
        }
    }

    pub fn awaiting_stop(
        &self,
        meta_stop: Option<broadcast::Sender<bool>>,
        mqtt_stop: Option<broadcast::Sender<bool>>,
        engine_stop: Option<broadcast::Sender<bool>>,
    ) {
        self.server_runtime.block_on(Box::pin(async {
            self.broker_cache
                .set_status(common_base::node_status::NodeStatus::Running)
                .await;
            signal::ctrl_c().await.expect("failed to listen for event");
            info!(
                "{}",
                "When ctrl + c is received, the service starts to stop"
            );

            self.broker_cache
                .set_status(common_base::node_status::NodeStatus::Stopping)
                .await;

            if let Some(sx) = mqtt_stop {
                if let Err(e) = sx.send(true) {
                    error!("mqtt stop signal, error message:{}", e);
                }
                sleep(Duration::from_secs(3));
            }

            if let Err(e) = self.offset_manager.flush().await {
                error!(
                    "Offset manager flush operation failed. Error message: {}",
                    e
                );
            }

            if let Some(sx) = engine_stop {
                if let Err(e) = sx.send(true) {
                    error!("storage engine stop signal, error message{}", e);
                }
                sleep(Duration::from_secs(3));
            }

            if let Some(sx) = meta_stop {
                if let Err(e) = sx.send(true) {
                    error!("meta stop signal, error message{}", e);
                }
            }

            if let Err(e) = self.delay_task_manager.stop().await {
                error!("delay task stop signal, error message{}", e);
            }

            sleep(Duration::from_secs(3));
        }));
    }

    fn check_grpc_server_ready(&self, grpc_port: u32, grpc_ready: Arc<AtomicBool>) {
        let max_retries = 30;
        let retry_interval = Duration::from_millis(100);

        std::thread::spawn(move || {
            let addr = format!("127.0.0.1:{grpc_port}");

            for attempt in 1..=max_retries {
                match std::net::TcpStream::connect(&addr) {
                    Ok(_) => {
                        info!("GRPC server is ready on port {grpc_port}");
                        grpc_ready.store(true, Ordering::Relaxed);
                        return;
                    }
                    Err(e) => {
                        if attempt % 10 == 0 {
                            info!(
                                "GRPC server not ready yet (attempt {}/{}): {}",
                                attempt, max_retries, e
                            );
                        }
                    }
                }

                std::thread::sleep(retry_interval);
            }
            error!(
                "GRPC server failed to start within {} attempts",
                max_retries
            );
            std::process::exit(1);
        });
    }

    fn wait_for_grpc_ready(&self, grpc_ready: &Arc<AtomicBool>) {
        let max_wait_time = Duration::from_secs(10);
        let check_interval = Duration::from_millis(100);
        let start_time = std::time::Instant::now();

        while !grpc_ready.load(Ordering::Relaxed) {
            if start_time.elapsed() > max_wait_time {
                error!("GRPC server failed to start within {:?}", max_wait_time);
                std::process::exit(1);
            }
            std::thread::sleep(check_interval);
        }

        info!("GRPC server startup check completed");
    }

    fn wait_for_engine_ready(&self) {
        let engine_port = self.config.storage_runtime.tcp_port;
        let max_wait_time = Duration::from_secs(10);
        let check_interval = Duration::from_millis(100);
        let start_time = std::time::Instant::now();

        while start_time.elapsed() < max_wait_time {
            match std::net::TcpStream::connect(format!("127.0.0.1:{engine_port}")) {
                Ok(_) => {
                    info!("Storage Engine startup check completed");
                    return;
                }
                Err(_) => {
                    std::thread::sleep(check_interval);
                }
            }
        }

        error!("Storage Engine failed to start within {:?}", max_wait_time);
        std::process::exit(1);
    }

    async fn register_node(&self, main_stop: broadcast::Sender<bool>) {
        // register node
        let client_pool = self.client_pool.clone();
        let broker_cache = self.broker_cache.clone();

        // register node
        let config = broker_config();
        match register_node(&client_pool, &broker_cache).await {
            Ok(()) => {
                // heartbeat report
                let raw_client_pool = client_pool.clone();
                tokio::spawn(Box::pin(async move {
                    report_heartbeat(&raw_client_pool, &broker_cache, main_stop.clone()).await;
                }));

                info!("Node {} has been successfully registered", config.broker_id);
            }
            Err(e) => {
                error!("Node registration failed. Error message:{}", e);
            }
        }
    }
}
